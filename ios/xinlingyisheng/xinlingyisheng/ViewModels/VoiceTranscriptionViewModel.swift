import Foundation
import AVFoundation
import Combine

@MainActor
class VoiceTranscriptionViewModel: ObservableObject {
    // MARK: - Published Properties
    @Published var isRecording: Bool = false
    @Published var isTranscribing: Bool = false
    @Published var transcriptionResult: TranscribeResponse?
    @Published var transcribedText: String = ""
    @Published var extractedSymptoms: [String] = []
    @Published var errorMessage: String?
    @Published var recordingDuration: TimeInterval = 0
    @Published var audioLevel: Float = 0

    // MARK: - Private Properties
    private var audioRecorder: AVAudioRecorder?
    private var recordingTimer: Timer?
    private var levelTimer: Timer?
    private var recordingURL: URL?

    // MARK: - Recording Methods

    func startRecording() {
        // è¯·æ±‚éº¦å…‹é£Žæƒé™
        if #available(iOS 17.0, *) {
            AVAudioApplication.requestRecordPermission { [weak self] allowed in
                Task { @MainActor in
                    if allowed {
                        await self?.beginRecording()
                    } else {
                        self?.errorMessage = "éœ€è¦éº¦å…‹é£Žæƒé™æ‰èƒ½å½•éŸ³"
                    }
                }
            }
        } else {
            AVAudioSession.sharedInstance().requestRecordPermission { [weak self] allowed in
                Task { @MainActor in
                    if allowed {
                        await self?.beginRecording()
                    } else {
                        self?.errorMessage = "éœ€è¦éº¦å…‹é£Žæƒé™æ‰èƒ½å½•éŸ³"
                    }
                }
            }
        }
    }

    private func beginRecording() async {
        let audioSession = AVAudioSession.sharedInstance()

        do {
            try audioSession.setCategory(.playAndRecord, mode: .default)
            try audioSession.setActive(true)

            // åˆ›å»ºå½•éŸ³æ–‡ä»¶è·¯å¾„
            let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            let fileName = "recording_\(Date().timeIntervalSince1970).m4a"
            recordingURL = documentsPath.appendingPathComponent(fileName)

            guard let url = recordingURL else { return }

            // å½•éŸ³è®¾ç½®
            let settings: [String: Any] = [
                AVFormatIDKey: Int(kAudioFormatMPEG4AAC),
                AVSampleRateKey: 44100,
                AVNumberOfChannelsKey: 1,
                AVEncoderAudioQualityKey: AVAudioQuality.high.rawValue
            ]

            audioRecorder = try AVAudioRecorder(url: url, settings: settings)
            audioRecorder?.isMeteringEnabled = true
            audioRecorder?.record()

            isRecording = true
            recordingDuration = 0
            errorMessage = nil

            // å¯åŠ¨è®¡æ—¶å™¨
            recordingTimer = Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { [weak self] _ in
                Task { @MainActor [weak self] in
                    guard let self else { return }
                    self.recordingDuration += 0.1
                }
            }

            // å¯åŠ¨éŸ³é‡ç›‘æµ‹
            levelTimer = Timer.scheduledTimer(withTimeInterval: 0.05, repeats: true) { [weak self] _ in
                Task { @MainActor [weak self] in
                    guard let self else { return }
                    self.audioRecorder?.updateMeters()
                    let level = self.audioRecorder?.averagePower(forChannel: 0) ?? -160
                    let normalizedLevel = max(0, (level + 60) / 60)
                    self.audioLevel = normalizedLevel
                }
            }

            print("[Voice] ðŸŽ™ï¸ Recording started")

        } catch {
            errorMessage = "å½•éŸ³å¤±è´¥: \(error.localizedDescription)"
            print("[Voice] âŒ Recording error: \(error)")
        }
    }

    func stopRecording() {
        audioRecorder?.stop()
        recordingTimer?.invalidate()
        levelTimer?.invalidate()
        recordingTimer = nil
        levelTimer = nil
        isRecording = false
        audioLevel = 0

        print("[Voice] ðŸŽ™ï¸ Recording stopped, duration: \(recordingDuration)s")

        // è‡ªåŠ¨è½¬å†™
        if let url = recordingURL {
            Task {
                await transcribeRecording(at: url)
            }
        }
    }

    func cancelRecording() {
        audioRecorder?.stop()
        recordingTimer?.invalidate()
        levelTimer?.invalidate()
        recordingTimer = nil
        levelTimer = nil
        isRecording = false
        audioLevel = 0
        recordingDuration = 0

        // åˆ é™¤å½•éŸ³æ–‡ä»¶
        if let url = recordingURL {
            try? FileManager.default.removeItem(at: url)
        }
        recordingURL = nil

        print("[Voice] ðŸŽ™ï¸ Recording cancelled")
    }

    // MARK: - Transcription Methods

    private func transcribeRecording(at url: URL) async {
        isTranscribing = true
        errorMessage = nil

        do {
            let audioData = try Data(contentsOf: url)
            let fileName = url.lastPathComponent

            print("[Voice] ðŸ“¤ Uploading audio for transcription...")

            let response = try await AIService.shared.transcribeAudioFile(
                audioData: audioData,
                fileName: fileName,
                language: "zh",
                extractSymptoms: true
            )

            transcriptionResult = response
            transcribedText = response.text ?? ""
            extractedSymptoms = response.extracted_symptoms ?? []

            print("[Voice] âœ… Transcription completed: \(transcribedText.prefix(50))...")

            // åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            try? FileManager.default.removeItem(at: url)

        } catch {
            errorMessage = "è½¬å†™å¤±è´¥: \(error.localizedDescription)"
            print("[Voice] âŒ Transcription error: \(error)")
        }

        isTranscribing = false
    }

    /// ä½¿ç”¨ Base64 è½¬å†™
    func transcribeBase64(_ base64String: String) async {
        isTranscribing = true
        errorMessage = nil

        do {
            let response = try await AIService.shared.transcribeAudioBase64(
                audioBase64: base64String,
                language: "zh",
                extractSymptoms: true
            )

            transcriptionResult = response
            transcribedText = response.text ?? ""
            extractedSymptoms = response.extracted_symptoms ?? []

        } catch {
            errorMessage = "è½¬å†™å¤±è´¥: \(error.localizedDescription)"
        }

        isTranscribing = false
    }

    /// ä½¿ç”¨ URL è½¬å†™
    func transcribeURL(_ audioUrl: String) async {
        isTranscribing = true
        errorMessage = nil

        do {
            let response = try await AIService.shared.transcribeAudioURL(
                audioUrl: audioUrl,
                language: "zh",
                extractSymptoms: true
            )

            transcriptionResult = response
            transcribedText = response.text ?? ""
            extractedSymptoms = response.extracted_symptoms ?? []

        } catch {
            errorMessage = "è½¬å†™å¤±è´¥: \(error.localizedDescription)"
        }

        isTranscribing = false
    }

    /// èŽ·å–è½¬å†™ä»»åŠ¡çŠ¶æ€ï¼ˆè½®è¯¢ç”¨ï¼‰
    func checkTranscriptionStatus(taskId: String) async -> TranscribeStatusResponse? {
        do {
            return try await AIService.shared.getTranscriptionStatus(taskId: taskId)
        } catch {
            print("[Voice] Failed to check status: \(error)")
            return nil
        }
    }

    // MARK: - Utility

    func reset() {
        transcriptionResult = nil
        transcribedText = ""
        extractedSymptoms = []
        errorMessage = nil
        recordingDuration = 0
    }

    var formattedDuration: String {
        let minutes = Int(recordingDuration) / 60
        let seconds = Int(recordingDuration) % 60
        return String(format: "%02d:%02d", minutes, seconds)
    }
}


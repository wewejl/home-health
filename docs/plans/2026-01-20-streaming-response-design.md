# æµå¼å“åº”æŠ€æœ¯è®¾è®¡æ–‡æ¡£

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**åˆ›å»ºæ—¥æœŸ**: 2026-01-20
**çŠ¶æ€**: å¾…å®æ–½
**ä¼˜å…ˆçº§**: ä¸­

---

## ä¸€ã€èƒŒæ™¯

### 1.1 é—®é¢˜æ¦‚è¿°

å½“å‰æ™ºèƒ½ä½“ç³»ç»Ÿè™½ç„¶æ”¯æŒ SSE (Server-Sent Events) è¿æ¥ï¼Œä½† AI å“åº”æ˜¯**ä¸€æ¬¡æ€§è¿”å›**çš„ï¼Œæ²¡æœ‰å®ç°çœŸæ­£çš„æµå¼è¾“å‡ºã€‚ç”¨æˆ·éœ€è¦ç­‰å¾… 4-8 ç§’æ‰èƒ½çœ‹åˆ°å®Œæ•´çš„ AI å›å¤ï¼Œä½“éªŒä¸ä½³ã€‚

### 1.2 å½“å‰ SSE è¾“å‡ºç¤ºä¾‹

```
event: meta
data: {"session_id": "...", "agent_type": "general"}

event: complete
data: {"message": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ‚¨çš„AIåŒ»ç”ŸåŠ©æ‰‹..."}
```

**é—®é¢˜**: ç¼ºå°‘ `event: chunk`ï¼Œæ— æ³•å®ç°æ‰“å­—æœºæ•ˆæœã€‚

### 1.3 æœŸæœ› SSE è¾“å‡º

```
event: meta
data: {"session_id": "...", "agent_type": "general"}

event: chunk
data: {"text": "æ‚¨å¥½"}

event: chunk
data: {"text": "ï¼Œæˆ‘æ˜¯"}

event: chunk
data: {"text": "æ‚¨çš„"}

event: chunk
data: {"text": "AIåŒ»ç”Ÿ"}

...

event: complete
data: {"message": "...", "stage": "...", ...}
```

---

## äºŒã€æ¶æ„åˆ†æ

### 2.1 å½“å‰è°ƒç”¨é“¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     sessions.py (è·¯ç”±å±‚)                      â”‚
â”‚  stream_agent_response()                                     â”‚
â”‚     â†“                                                        â”‚
â”‚     chunk_queue = asyncio.Queue()                           â”‚
â”‚     on_chunk = lambda c: chunk_queue.put(("chunk", c))      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ReActAgent (æ™ºèƒ½ä½“å±‚)                       â”‚
â”‚  run(state, user_input, on_chunk, ...)                      â”‚
â”‚     â†“                                                        â”‚
â”‚  graph.ainvoke(state)  â† éæµå¼ï¼                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LangGraph (å›¾æ‰§è¡Œå¼•æ“)                    â”‚
â”‚  _reasoning_node()          â† ä½¿ç”¨ chat_with_tools()        â”‚
â”‚     â†“                                                        â”‚
â”‚  _tool_executor_node()      â† æ‰§è¡Œå·¥å…·                       â”‚
â”‚     â†“                                                        â”‚
â”‚  _response_generator_node() â† ä½¿ç”¨ chat_with_tools()        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QwenService (LLM å±‚)                     â”‚
â”‚  âœ“ chat_with_tools()         - éæµå¼                        â”‚
â”‚  âœ“ chat_with_tools_stream() - æµå¼ (å·²å­˜åœ¨ä½†æœªä½¿ç”¨!)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 é—®é¢˜æ ¹æº

| å±‚çº§ | é—®é¢˜ | è¯´æ˜ |
|------|------|------|
| æ™ºèƒ½ä½“å±‚ | `run()` ä½¿ç”¨ `ainvoke()` | LangGraph çš„ `ainvoke` æ˜¯éæµå¼çš„ |
| èŠ‚ç‚¹å±‚ | `_reasoning_node` ä½¿ç”¨éæµå¼ LLM | è°ƒç”¨ `chat_with_tools()` è€Œé `chat_with_tools_stream()` |
| èŠ‚ç‚¹å±‚ | `_response_generator_node` ä½¿ç”¨éæµå¼ LLM | åŒä¸Š |
| å›è°ƒå±‚ | `on_chunk` ä»æœªè¢«è°ƒç”¨ | è™½ç„¶ä¼ é€’äº†ä½†å†…éƒ¨æ²¡ä½¿ç”¨ |

### 2.3 ç°æœ‰å¯ç”¨èµ„æº

```python
# qwen_service.py - å·²å­˜åœ¨ä½†æœªä½¿ç”¨!
async def chat_with_tools_stream(
    messages, tools, tool_choice, model, temperature, max_tokens
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    Yields:
        {"type": "content", "delta": "æ–‡æœ¬ç‰‡æ®µ"}
        {"type": "tool_call", "tool_call": {...}}
        {"type": "done", "finish_reason": "..."}
    """
```

---

## ä¸‰ã€è®¾è®¡æ–¹æ¡ˆ

### 3.1 ä¿®æ”¹ç­–ç•¥

é‡‡ç”¨ **å…¨æµç¨‹æµå¼** æ–¹æ¡ˆï¼Œè®©ç”¨æˆ·çœ‹åˆ° AI çš„å®Œæ•´æ€è€ƒè¿‡ç¨‹ï¼š

1. **æ¨ç†é˜¶æ®µ**: æµå¼è¾“å‡º AI çš„æ€è€ƒè¿‡ç¨‹ï¼ˆå¯é€‰ï¼Œé€šè¿‡é…ç½®æ§åˆ¶ï¼‰
2. **å·¥å…·æ‰§è¡Œ**: æ˜¾ç¤º"æ­£åœ¨åˆ†æ..."ç­‰çŠ¶æ€
3. **å“åº”ç”Ÿæˆ**: æµå¼è¾“å‡ºæœ€ç»ˆå›å¤

### 3.2 æ–°çš„è°ƒç”¨é“¾

```
stream_agent_response()
    â†“
ReActAgent.run_stream()  â† æ–°æ–¹æ³•
    â†“
graph.astream()  â† æµå¼å›¾éå†
    â†“
    â”œâ”€ reasoning_node_stream()  â† æµå¼æ¨ç†
    â”œâ”€ tool_executor_node()
    â””â”€ response_generator_node_stream()  â† æµå¼ç”Ÿæˆ
    â†“
on_chunk("...")  â† é€å­—è¾“å‡º
```

### 3.3 æ–°å¢äº‹ä»¶ç±»å‹

```python
# å½“å‰äº‹ä»¶ç±»å‹
event: chunk      # æ–‡æœ¬ç‰‡æ®µ
event: complete  # å®Œæˆ

# æ–°å¢äº‹ä»¶ç±»å‹
event: thinking   # AI æ€è€ƒä¸­
event: tool_call  # è°ƒç”¨å·¥å…·
event: tool_result # å·¥å…·ç»“æœ
```

---

## å››ã€å…·ä½“æ”¹åŠ¨

### 4.1 ä¿®æ”¹ `react_base.py`

#### 4.1.1 æ–°å¢ `run_stream()` æ–¹æ³•

**ä½ç½®**: `ReActAgent` ç±»ä¸­ï¼Œä¸ `run()` å¹¶åˆ—

```python
async def run_stream(
    self,
    state: Dict[str, Any],
    user_input: str = None,
    attachments: list = None,
    action: str = "conversation",
    on_chunk: Optional[Callable[[str], Awaitable[None]]] = None,
    show_thinking: bool = False,  # æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
    **kwargs
) -> AgentResponse:
    """
    æµå¼è¿è¡Œ ReAct Agent

    Args:
        state: å½“å‰ä¼šè¯çŠ¶æ€
        user_input: ç”¨æˆ·è¾“å…¥
        attachments: é™„ä»¶åˆ—è¡¨
        action: åŠ¨ä½œç±»å‹
        on_chunk: æµå¼è¾“å‡ºå›è°ƒ
        show_thinking: æ˜¯å¦æ˜¾ç¤º AI æ€è€ƒè¿‡ç¨‹

    Returns:
        AgentResponse
    """
    # é‡ç½®çŠ¶æ€
    state["iteration_count"] = 0
    state["tool_results"] = []
    state["pending_tool_calls"] = []

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    if user_input:
        state["messages"].append({"role": "user", "content": user_input})

    if attachments:
        state["attachments"] = attachments

    try:
        # ä½¿ç”¨ astream è€Œä¸æ˜¯ ainvoke
        async for event in self.graph.astream(state):
            node_name = event.get("node", "")
            node_state = event

            # å¤„ç†ä¸åŒèŠ‚ç‚¹
            if node_name == "reasoning":
                await self._handle_reasoning_stream(
                    node_state, on_chunk, show_thinking
                )
            elif node_name == "response_generator":
                await self._handle_response_stream(
                    node_state, on_chunk
                )

        # è·å–æœ€ç»ˆçŠ¶æ€
        final_state = state
        serialized_state = self._serialize_state_for_db(final_state)

        return AgentResponse(
            message=final_state.get("current_response", ""),
            stage=final_state.get("stage", "collecting"),
            progress=final_state.get("progress", 0),
            quick_options=final_state.get("quick_options", []),
            risk_level=final_state.get("risk_level"),
            specialty_data=final_state.get("specialty_data"),
            next_state=serialized_state
        )

    except Exception as e:
        import traceback
        traceback.print_exc()
        return AgentResponse(
            message=f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜: {str(e)}",
            stage=state.get("stage", "collecting"),
            progress=state.get("progress", 0),
            quick_options=[],
            next_state=state
        )
```

#### 4.1.2 æ–°å¢ `_handle_reasoning_stream()` æ–¹æ³•

```python
async def _handle_reasoning_stream(
    self,
    state: Dict[str, Any],
    on_chunk: Optional[Callable[[str], Awaitable[None]]],
    show_thinking: bool
):
    """å¤„ç†æ¨ç†èŠ‚ç‚¹çš„æµå¼è¾“å‡º"""
    messages = self._build_reasoning_messages(state)
    decision_instruction = self._get_decision_instruction(state)
    messages.append({"role": "user", "content": decision_instruction})

    # å‘é€æ€è€ƒçŠ¶æ€
    if show_thinking and on_chunk:
        await on_chunk("ğŸ¤” æ­£åœ¨åˆ†æ...\n")

    # ä½¿ç”¨æµå¼ LLM
    full_response = ""
    async for chunk in QwenService.chat_with_tools_stream(
        messages=messages,
        tools=self._tool_schemas,
        tool_choice="auto",
        max_tokens=2000
    ):
        chunk_type = chunk.get("type")

        if chunk_type == "content":
            delta = chunk.get("delta", "")
            full_response += delta
            # å¦‚æœæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹ï¼Œå‘é€ç»™å‰ç«¯
            if show_thinking and on_chunk:
                await on_chunk(delta)

        elif chunk_type == "tool_call":
            # AI å†³å®šè°ƒç”¨å·¥å…·
            tool_call = chunk.get("tool_call", {})
            state["pending_tool_calls"] = [tool_call]
            state["agent_decision"] = {
                "action": "use_tool",
                "tool_calls": [tool_call],
                "thought": full_response
            }
            return

        elif chunk_type == "done":
            # AI å†³å®šç›´æ¥å›å¤
            decision = self._parse_decision(full_response)
            state["agent_decision"] = decision
            return
```

#### 4.1.3 æ–°å¢ `_handle_response_stream()` æ–¹æ³•

```python
async def _handle_response_stream(
    self,
    state: Dict[str, Any],
    on_chunk: Optional[Callable[[str], Awaitable[None]]]
):
    """å¤„ç†å“åº”ç”ŸæˆèŠ‚ç‚¹çš„æµå¼è¾“å‡º"""
    decision = state.get("agent_decision", {})
    response = decision.get("response", "")

    if not response:
        action = decision.get("action", "respond")
        if action == "diagnose":
            # æµå¼ç”Ÿæˆè¯Šæ–­
            response = await self._generate_diagnosis_stream(state, on_chunk)
        else:
            response = "è¯·é—®è¿˜æœ‰ä»€ä¹ˆéœ€è¦äº†è§£çš„å—ï¼Ÿ"

    # æµå¼è¾“å‡ºå“åº”
    if on_chunk:
        for char in response:
            await on_chunk(char)

    state["current_response"] = response
    state["quick_options"] = decision.get("quick_options", [])

    if decision.get("stage"):
        state["stage"] = decision["stage"]
    if decision.get("progress"):
        state["progress"] = decision["progress"]
```

#### 4.1.4 æ–°å¢ `_generate_diagnosis_stream()` æ–¹æ³•

```python
async def _generate_diagnosis_stream(
    self,
    state: Dict[str, Any],
    on_chunk: Optional[Callable[[str], Awaitable[None]]]
) -> str:
    """æµå¼ç”Ÿæˆè¯Šæ–­"""
    ctx = state.get("medical_context", {})
    specialty_data = state.get("specialty_data", {})

    diagnosis_prompt = f"""åŸºäºä»¥ä¸‹æ”¶é›†çš„ä¿¡æ¯ï¼Œè¯·ç»™å‡ºä¸“ä¸šçš„åˆæ­¥è¯Šæ–­æ„è§ï¼š

- {self._format_medical_context(ctx)}

å›¾åƒåˆ†æç»“æœï¼š{specialty_data.get('skin_analysis', {}).get('findings', 'æ— ')}
é£é™©è¯„ä¼°ï¼š{specialty_data.get('risk_assessment', {}).get('risk_level', 'æœªè¯„ä¼°')}

è¯·åŒ…å«ï¼š
1. å¯èƒ½çš„è¯Šæ–­ï¼ˆæŒ‰å¯èƒ½æ€§æ’åºï¼‰
2. è¯Šæ–­ä¾æ®
3. å»ºè®®çš„å¤„ç†æ–¹å¼
4. æ˜¯å¦éœ€è¦çº¿ä¸‹å°±åŒ»
5. æ—¥å¸¸æŠ¤ç†å»ºè®®
6. æ³¨æ„äº‹é¡¹å’Œè­¦ç¤ºä¿¡å·

è¯·ç”¨ä¸“ä¸šä½†é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å›å¤ã€‚"""

    messages = [
        {"role": "system", "content": self.get_system_prompt()},
        {"role": "user", "content": diagnosis_prompt}
    ]

    full_response = ""
    async for chunk in QwenService.chat_with_tools_stream(
        messages=messages,
        tools=[],
        max_tokens=2000
    ):
        if chunk.get("type") == "content":
            delta = chunk.get("delta", "")
            full_response += delta
            if on_chunk:
                await on_chunk(delta)
        elif chunk.get("type") == "done":
            break

    return full_response
```

### 4.2 ä¿®æ”¹ `sessions.py` è·¯ç”±

#### 4.2.1 ä¿®æ”¹ `stream_agent_response()` å‡½æ•°

**ä½ç½®**: `sessions.py`, çº¦ 327 è¡Œ

**å½“å‰ä»£ç **:
```python
final_state = await agent.run(
    state=state,
    user_input=user_input,
    attachments=attachments,
    action=action,
    on_chunk=on_chunk,
    **extra_kwargs
)
```

**ä¿®æ”¹ä¸º**:
```python
# æ£€æŸ¥æ˜¯å¦æ”¯æŒæµå¼
if hasattr(agent, 'run_stream'):
    final_state = await agent.run_stream(
        state=state,
        user_input=user_input,
        attachments=attachments,
        action=action,
        on_chunk=on_chunk,
        show_thinking=False,  # å¯é…ç½®
        **extra_kwargs
    )
else:
    # é™çº§åˆ°éæµå¼
    final_state = await agent.run(
        state=state,
        user_input=user_input,
        attachments=attachments,
        action=action,
        on_chunk=on_chunk,
        **extra_kwargs
    )
```

### 4.3 æ–°å¢é…ç½®é¡¹

åœ¨ `config.py` ä¸­æ·»åŠ :

```python
# æµå¼å“åº”é…ç½®
ENABLE_STREAMING: bool = Field(default=True)
SHOW_THINKING_PROCESS: bool = Field(default=False)  # æ˜¯å¦æ˜¾ç¤º AI æ€è€ƒè¿‡ç¨‹
STREAMING_CHUNK_SIZE: int = Field(default=10)  # æ¯æ¬¡å‘é€çš„å­—ç¬¦æ•°
```

---

## äº”ã€å‰ç«¯å¯¹æ¥

### 5.1 å½“å‰å‰ç«¯ SSE å¤„ç†

```typescript
// éœ€è¦æ”¯æŒæ–°çš„äº‹ä»¶ç±»å‹
const eventHandlers = {
  meta: (data) => { /* ... */ },
  chunk: (data) => {
    // å½“å‰: data.text
    // éœ€è¦å¤„ç†: æ€è€ƒã€å·¥å…·è°ƒç”¨ç­‰
  },
  complete: (data) => { /* ... */ },
  // æ–°å¢
  thinking: (data) => { /* æ˜¾ç¤ºæ€è€ƒåŠ¨ç”» */ },
  tool_call: (data) => { /* æ˜¾ç¤ºå·¥å…·è°ƒç”¨çŠ¶æ€ */ },
  tool_result: (data) => { /* æ˜¾ç¤ºå·¥å…·ç»“æœ */ },
};
```

### 5.2 å»ºè®®çš„ UI æ”¹è¿›

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ‘¤ ç”¨æˆ·: æˆ‘å¤´ç—›ï¼Œè¯·é—®æˆ‘è¯¥æ€ä¹ˆåŠï¼Ÿ      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ¤” AI æ­£åœ¨åˆ†æ...                      â”‚  â† æ–°å¢æ€è€ƒçŠ¶æ€
â”‚                                         â”‚
â”‚ [âœ“ æŸ¥è¯¢åŒ»å­¦çŸ¥è¯†]                       â”‚  â† æ–°å¢å·¥å…·è°ƒç”¨çŠ¶æ€
â”‚                                         â”‚
â”‚ æ ¹æ®æ‚¨çš„æè¿°ï¼Œå¤´ç—›å¯èƒ½ç”±å¤šç§åŸå›         â”‚  â† æµå¼è¾“å‡º
â”‚ å¼•èµ·ã€‚è¯·é—®æ‚¨çš„å¤´ç—›æ˜¯æœ€è¿‘æ‰å¼€å§‹çš„å—      â”‚
â”‚ ï¼Ÿå…·ä½“åœ¨å“ªä¸ªéƒ¨ä½ï¼Ÿ                      â”‚
â”‚                                         â”‚
â”‚ [æ„Ÿå†’å‘çƒ­] [æ¶ˆåŒ–ä¸é€‚] [å…¶ä»–ç—‡çŠ¶]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å…­ã€æµ‹è¯•è®¡åˆ’

### 6.1 å•å…ƒæµ‹è¯•

```python
# test_streaming_agent.py

async def test_run_stream_basic():
    """æµ‹è¯•åŸºç¡€æµå¼è¾“å‡º"""
    agent = GeneralReActAgent()
    state = create_initial_state("test-session", 1, "general")

    chunks = []
    async def on_chunk(chunk):
        chunks.append(chunk)

    response = await agent.run_stream(
        state=state,
        user_input="ä½ å¥½",
        on_chunk=on_chunk
    )

    assert len(chunks) > 0
    assert response.message is not None

async def test_run_stream_with_tools():
    """æµ‹è¯•å·¥å…·è°ƒç”¨çš„æµå¼è¾“å‡º"""
    # ...

async def test_run_stream_with_thinking():
    """æµ‹è¯•æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹"""
    # ...
```

### 6.2 é›†æˆæµ‹è¯•

```bash
# 1. å¯åŠ¨åç«¯
curl -s 'http://localhost:8100/health'

# 2. åˆ›å»ºä¼šè¯å¹¶æµ‹è¯•æµå¼
curl -N -X POST 'http://localhost:8100/sessions/{id}/messages' \
  -H 'Accept: text/event-stream' \
  -d '{"content": "ä½ å¥½"}'

# é¢„æœŸ: çœ‹åˆ° event: chunk äº‹ä»¶
```

### 6.3 éªŒæ”¶æ ‡å‡†

- [ ] SSE å“åº”åŒ…å« `event: chunk` äº‹ä»¶
- [ ] chunk äº‹ä»¶åŒ…å«éç©º `text` å­—æ®µ
- [ ] å‰ç«¯èƒ½æ­£ç¡®æ˜¾ç¤ºé€å­—è¾“å‡ºæ•ˆæœ
- [ ] å·¥å…·è°ƒç”¨æ—¶æ˜¾ç¤ºå¯¹åº”çŠ¶æ€
- [ ] å®ŒæˆååŒ…å«å®Œæ•´å“åº”
- [ ] é”™è¯¯æƒ…å†µèƒ½æ­£ç¡®å¤„ç†

---

## ä¸ƒã€å®æ–½æ­¥éª¤

### é˜¶æ®µ 1: åŸºç¡€æµå¼ (1-2 å¤©)

1. åœ¨ `react_base.py` æ–°å¢ `run_stream()` æ–¹æ³•
2. ä¿®æ”¹ `_response_generator_node` æ”¯æŒæµå¼
3. ä¿®æ”¹ `sessions.py` è°ƒç”¨æ–°æ–¹æ³•
4. æµ‹è¯•éªŒè¯åŸºç¡€æµå¼è¾“å‡º

### é˜¶æ®µ 2: å®Œæ•´æµå¼ (2-3 å¤©)

1. å®ç° `_handle_reasoning_stream()`
2. å®ç° `_generate_diagnosis_stream()`
3. æ·»åŠ å·¥å…·è°ƒç”¨çŠ¶æ€è¾“å‡º
4. å®Œæ•´æµ‹è¯•

### é˜¶æ®µ 3: å‰ç«¯å¯¹æ¥ (2-3 å¤©)

1. æ›´æ–° SSE äº‹ä»¶å¤„ç†
2. æ·»åŠ æ€è€ƒçŠ¶æ€ UI
3. æ·»åŠ å·¥å…·è°ƒç”¨çŠ¶æ€ UI
4. è”è°ƒæµ‹è¯•

### é˜¶æ®µ 4: ä¼˜åŒ– (1 å¤©)

1. æ€§èƒ½ä¼˜åŒ–
2. é”™è¯¯å¤„ç†å®Œå–„
3. é…ç½®é¡¹è°ƒæ•´
4. æ–‡æ¡£æ›´æ–°

**é¢„è®¡æ€»å·¥æ—¶**: 6-9 å¤©

---

## å…«ã€é£é™©è¯„ä¼°

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| LangGraph astream å…¼å®¹æ€§ | ä¸­ | å…ˆåšæŠ€æœ¯éªŒè¯ï¼Œç¡®ä¿ astream è¡Œä¸ºç¬¦åˆé¢„æœŸ |
| æµå¼ä¸çŠ¶æ€åŒæ­¥ | é«˜ | ç¡®ä¿æµå¼è¾“å‡ºåçŠ¶æ€æ­£ç¡®æ›´æ–° |
| æ€§èƒ½é—®é¢˜ | ä¸­ | æ·»åŠ ç¼“å­˜ï¼Œæ§åˆ¶æµå¼é¢‘ç‡ |
| å‰ç«¯å…¼å®¹æ€§ | ä½ | ä¿ç•™éæµå¼é™çº§æ–¹æ¡ˆ |

---

## ä¹ã€å®æ–½çŠ¶æ€

### âœ… å·²å®Œæˆ

#### é˜¶æ®µ 1: åŸºç¡€æµå¼
- [x] åœ¨ `react_base.py` æ–°å¢ `run_stream()` æ–¹æ³•
- [x] ä¿®æ”¹ `_response_generator_node` æ”¯æŒæµå¼
- [x] ä¿®æ”¹ `sessions.py` è°ƒç”¨æ–°æ–¹æ³•
- [x] æµ‹è¯•éªŒè¯åŸºç¡€æµå¼è¾“å‡º

#### é˜¶æ®µ 2: å®Œæ•´æµå¼
- [x] å®ç° `_handle_reasoning_stream()`
- [x] å®ç° `_generate_diagnosis_stream()`
- [x] æ·»åŠ å·¥å…·è°ƒç”¨çŠ¶æ€è¾“å‡º
- [x] å®Œæ•´æµ‹è¯•

#### é˜¶æ®µ 3: å‰ç«¯å¯¹æ¥
- [x] iOS: æ›´æ–° SSE äº‹ä»¶å¤„ç† (`APITypes.swift`, `UnifiedChatAPIService.swift`)
- [x] iOS: æ·»åŠ  `@Published` çŠ¶æ€å±æ€§ (`UnifiedChatViewModel.swift`)
- [x] iOS: æ·»åŠ  `StreamingStatusView` ç»„ä»¶
- [x] React: æ›´æ–° SSE äº‹ä»¶å¤„ç† (`api/index.ts`)
- [x] React: æ·»åŠ  `StreamingStatusView` ç»„ä»¶ (`DermaChat.tsx`)

#### é˜¶æ®µ 4: ä¼˜åŒ–
- [x] æ€§èƒ½ä¼˜åŒ–: åˆ†å—ç¼“å†² (`_stream_chunked`)
- [x] é”™è¯¯å¤„ç†: è¶…æ—¶å¤„ç† (`asyncio.wait_for`)
- [x] é…ç½®é¡¹: æ·»åŠ  `STREAMING_TIMEOUT`, `STREAMING_QUEUE_SIZE`
- [x] æ–‡æ¡£æ›´æ–°

### å…³é”®å®ç°æ–‡ä»¶

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ |
|------|---------|
| `backend/app/services/agents/react_base.py` | æ·»åŠ  `run_stream()`, `_handle_reasoning_stream()`, `_generate_diagnosis_stream()`, `_stream_chunked()` |
| `backend/app/routes/sessions.py` | æ·»åŠ è¶…æ—¶å¤„ç†ã€é˜Ÿåˆ—å¤§å°é™åˆ¶ã€é”™è¯¯æ¢å¤ |
| `backend/app/config.py` | æ·»åŠ æµå¼é…ç½®é¡¹ |
| `ios/.../Components/StreamingStatusView.swift` | æ–°å¢æµå¼çŠ¶æ€ UI ç»„ä»¶ |
| `ios/.../ViewModels/UnifiedChatViewModel.swift` | æ·»åŠ  `isThinking`, `activeToolCalls`, `completedTools` çŠ¶æ€ |
| `frontend/src/pages/DermaChat.tsx` | æ·»åŠ  `StreamingStatusView` ç»„ä»¶å’Œäº‹ä»¶å¤„ç† |

---

## åã€å‚è€ƒèµ„æ–™

- LangGraph æµå¼æ‰§è¡Œ: https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming
- Server-Sent Events: https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events
- é€šä¹‰åƒé—®æµå¼ API: https://help.aliyun.com/zh/dashscope/developer-reference/api-details

---

**æ–‡æ¡£ç»´æŠ¤**: æœ¬æ–‡æ¡£åº”åœ¨å®æ–½è¿‡ç¨‹ä¸­æŒç»­æ›´æ–°ï¼Œè®°å½•å®é™…é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚
